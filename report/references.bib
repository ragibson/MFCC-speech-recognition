
@inproceedings{graves_speech_2013,
	title = {Speech recognition with deep recurrent neural networks},
	doi = {10.1109/ICASSP.2013.6638947},
	abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7\% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	author = {Graves, A. and Mohamed, A. and Hinton, G.},
	month = may,
	year = {2013},
	keywords = {Acoustics, connectionist temporal classification, deep neural networks, deep recurrent neural networks, end-to-end training methods, long short-term memory RNN architecture, Noise, recurrent neural networks, Recurrent neural networks, sequential data, speech recognition, Speech recognition, Training, Vectors},
	pages = {6645--6649}
}

@inproceedings{deng_new_2013,
	title = {New types of deep neural network learning for speech recognition and related applications: an overview},
	shorttitle = {New types of deep neural network learning for speech recognition and related applications},
	doi = {10.1109/ICASSP.2013.6639344},
	abstract = {In this paper, we provide an overview of the invited and contributed papers presented at the special session at ICASSP-2013, entitled “New Types of Deep Neural Network Learning for Speech Recognition and Related Applications,” as organized by the authors. We also describe the historical context in which acoustic models based on deep neural networks have been developed. The technical overview of the papers presented in our special session is organized into five ways of improving deep learning methods: (1) better optimization; (2) better types of neural activation function and better network architectures; (3) better ways to determine the myriad hyper-parameters of deep neural networks; (4) more appropriate ways to preprocess speech for deep neural networks; and (5) ways of leveraging multiple languages or dialects that are more easily achieved with deep neural networks than with Gaussian mixture models.},
	booktitle = {2013 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	author = {Deng, L. and Hinton, G. and Kingsbury, B.},
	month = may,
	year = {2013},
	keywords = {Acoustics, speech recognition, Speech recognition, Training, acoustic models, acoustic signal processing, convolutional neural network, deep neural network, deep neural network learning, dialects, Hidden Markov models, ICASSP- 2013, learning (artificial intelligence), multilingual, multiple languages, multitask, music processing, myriad hyper-parameter determination, network architectures, neural activation function, neural net architecture, Neural networks, optimisation, optimization, Optimization, parameter estimation, recurrent neural network, spectrogram features, Speech, speech preprocessing},
	pages = {8599--8603}
}

@article{deng_machine_2013,
	title = {Machine {Learning} {Paradigms} for {Speech} {Recognition}: {An} {Overview}},
	volume = {21},
	issn = {1558-7916},
	shorttitle = {Machine {Learning} {Paradigms} for {Speech} {Recognition}},
	doi = {10.1109/TASL.2013.2244083},
	abstract = {Automatic Speech Recognition (ASR) has historically been a driving force behind many machine learning (ML) techniques, including the ubiquitously used hidden Markov model, discriminative learning, structured sequence learning, Bayesian learning, and adaptive learning. Moreover, ML can and occasionally does use ASR as a large-scale, realistic application to rigorously test the effectiveness of a given technique, and to inspire new problems arising from the inherently sequential and dynamic nature of speech. On the other hand, even though ASR is available commercially for some applications, it is largely an unsolved problem - for almost all applications, the performance of ASR is not on par with human performance. New insight from modern ML methodology shows great promise to advance the state-of-the-art in ASR technology. This overview article provides readers with an overview of modern ML techniques as utilized in the current and as relevant to future ASR research and systems. The intent is to foster further cross-pollination between the ML and ASR communities than has occurred in the past. The article is organized according to the major ML paradigms that are either popular already or have potential for making significant contributions to ASR technology. The paradigms presented and elaborated in this overview include: generative and discriminative learning; supervised, unsupervised, semi-supervised, and active learning; adaptive and multi-task learning; and Bayesian learning. These learning paradigms are motivated and discussed in the context of ASR technology and applications. We finally present and analyze recent developments of deep learning and learning with sparse representations, focusing on their direct relevance to advancing ASR technology.},
	number = {5},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Deng, L. and Li, X.},
	month = may,
	year = {2013},
	keywords = {Acoustics, speech recognition, Speech recognition, Training, learning (artificial intelligence), active learning, adaptive, adaptive learning, ASR communities, ASR research, ASR systems, automatic speech recognition, Bayesian, Bayesian learning, Bayesian methods, cross-pollination, deep learning, discriminative, discriminative learning, dynamics, generative, generative learning, hidden Markov model, hidden Markov models, Machine learning, machine learning paradigms, machine learning techniques, multitask learning, overview, semisupervised learning, Speech processing, structured sequence learning, super vised, supervised learning, unsupervised, unsupervised learning},
	pages = {1060--1089}
}

@inproceedings{amodei_et_al_deep_2016,
	address = {New York, New York, USA},
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Deep {Speech} 2 : {End}-to-{End} {Speech} {Recognition} in {English} and {Mandarin}},
	volume = {48},
	url = {http://proceedings.mlr.press/v48/amodei16.html},
	abstract = {We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech–two vastly different languages. Because it replaces entire pipelines of hand-engineered components with neural networks, end-to-end learning allows us to handle a diverse variety of speech including noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, enabling experiments that previously took weeks to now run in days. This allows us to iterate more quickly to identify superior architectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, delivering low latency when serving users at scale.},
	booktitle = {Proceedings of {The} 33rd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Amodei et al, Dario},
	month = jun,
	year = {2016},
	pages = {173--182}
}

@article{sahidullah_design_2012,
	title = {Design, analysis and experimental evaluation of block based transformation in {MFCC} computation for speaker recognition},
	volume = {54},
	issn = {0167-6393},
	url = {http://www.sciencedirect.com/science/article/pii/S0167639311001622},
	doi = {10.1016/j.specom.2011.11.004},
	abstract = {Standard Mel frequency cepstrum coefficient (MFCC) computation technique utilizes discrete cosine transform (DCT) for decorrelating log energies of filter bank output. The use of DCT is reasonable here as the covariance matrix of Mel filter bank log energy (MFLE) can be compared with that of highly correlated Markov-I process. This full-band based MFCC computation technique where each of the filter bank output has contribution to all coefficients, has two main disadvantages. First, the covariance matrix of the log energies does not exactly follow Markov-I property. Second, full-band based MFCC feature gets severely degraded when speech signal is corrupted with narrow-band channel noise, though few filter bank outputs may remain unaffected. In this work, we have studied a class of linear transformation techniques based on block wise transformation of MFLE which effectively decorrelate the filter bank log energies and also capture speech information in an efficient manner. A thorough study has been carried out on the block based transformation approach by investigating a new partitioning technique that highlights associated advantages. This article also reports a novel feature extraction scheme which captures complementary information to wide band information; that otherwise remains undetected by standard MFCC and proposed block transform (BT) techniques. The proposed features are evaluated on NIST SRE databases using Gaussian mixture model-universal background model (GMM-UBM) based speaker recognition system. We have obtained significant performance improvement over baseline features for both matched and mismatched condition, also for standard and narrow-band noises. The proposed method achieves significant performance improvement in presence of narrow-band noise when clubbed with missing feature theory based score computation scheme.},
	number = {4},
	urldate = {2019-05-01},
	journal = {Speech Communication},
	author = {Sahidullah, Md. and Saha, Goutam},
	month = may,
	year = {2012},
	keywords = {Block transform, Correlation matrix, DCT, Decorrelation technique, Linear transformation, MFCC, Missing feature theory, Narrow-band noise, Speaker recognition},
	pages = {543--565}
}

@article{warden_speech_2017,
	title = {Speech {Commands}: {A} public dataset for single-word speech recognition.},
	journal = {Dataset available from http://download.tensorflow.org/data/speech\_commands\_v0.01.tar.gz},
	author = {Warden, Pete},
	year = {2017}
}

@article{kaggle.com_tensorflow_nodate,
	title = {{TensorFlow} {Speech} {Recognition} {Challenge}.},
	abstract = {Can you build an algorithm that understands simple speech commands?},
	urldate = {2019-05-02},
	journal = {https://kaggle.com/c/tensorflow-speech-recognition-challenge},
	author = {{Kaggle.com}}
}

@article{james_lyons_python_speech_features:_nodate,
	title = {python\_speech\_features: {A} library providing common speech features for {ASR} including {MFCCs} and filterbank energies.},
	url = {https://python-speech-features.readthedocs.io/en/latest/},
	urldate = {2019-05-02},
	journal = {https://github.com/jameslyons/python\_speech\_features},
	author = {{James Lyons}}
}

@inproceedings{paszke_automatic_2017,
	title = {Automatic differentiation in {PyTorch}},
	booktitle = {{NIPS}-{W}},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	year = {2017}
}

@article{duchi_adaptive_2011,
	title = {Adaptive {Subgradient} {Methods} for {Online} {Learning} and {Stochastic} {Optimization}},
	volume = {12},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=1953048.2021068},
	journal = {J. Mach. Learn. Res.},
	author = {Duchi, John and Hazan, Elad and Singer, Yoram},
	month = jul,
	year = {2011},
	pages = {2121--2159}
}

@article{glorot_understanding_2010,
	title = {Understanding the difficulty of training deep feedforward neural networks},
	volume = {9},
	journal = {Journal of Machine Learning Research - Proceedings Track},
	author = {Glorot, Xavier and Bengio, Y},
	year = {2010},
	pages = {249--256}
}

@article{xu_empirical_2015,
	title = {Empirical {Evaluation} of {Rectified} {Activations} in {Convolutional} {Network}},
	url = {http://arxiv.org/abs/1505.00853},
	abstract = {In this paper we investigate the performance of different types of rectified activation functions in convolutional neural network: standard rectified linear unit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified linear unit (PReLU) and a new randomized leaky rectified linear units (RReLU). We evaluate these activation function on standard image classification task. Our experiments suggest that incorporating a non-zero slope for negative part in rectified activation units could consistently improve the results. Thus our findings are negative on the common belief that sparsity is the key of good performance in ReLU. Moreover, on small scale dataset, using deterministic negative slope or learning it are both prone to overfitting. They are not as effective as using their randomized counterpart. By using RReLU, we achieved 75.68{\textbackslash}\% accuracy on CIFAR-100 test set without multiple test or ensemble.},
	urldate = {2019-05-02},
	journal = {arXiv:1505.00853 [cs, stat]},
	author = {Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
	month = may,
	year = {2015},
	note = {arXiv: 1505.00853},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning}
}
